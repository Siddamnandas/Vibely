"use client";

import { RedisQueueManager, QueueTask } from './redis-queue';

// AI Model Configuration
export interface AIModel {
  id: string;
  name: string;
  provider: 'replicate' | 'stability' | 'openai' | 'anthropic';
  cost: number; // Cost per second or request
  quality: number; // 1-100 quality score
  speed: 'slow' | 'medium' | 'fast';
  capabilities: string[];
  maxBatchSize: number;
  estimatedTime: number; // seconds
  retryCostMultiplier: number;
  compatibilityScore: number; // Compatibility with our use cases
}

// AI Pipeline Queue Configurations
export interface AIQueueConfig {
  priority: 'interactive' | 'background' | 'batch' | 'test';
  maxConcurrency: number;
  costBudget: number;
  qualityThreshold: number;
  timeout: number;
  maxRetries: number;
  models: AIModel[];
  queueBacklogLimit: number;
}

export interface AIPipelineMetrics {
  totalGenerations: number;
  averageQuality: number;
  averageCost: number;
  averageGenerationTime: number;
  successRate: number;
  queueWaitTime: number;
  costPerGeneration: number;
  modelUtilization: { [modelId: string]: number };
}

export interface SmartSuggestion {
  modelId: string;
  confidence: number;
  reasons: string[];
  expectedCost: number;
  expectedQuality: number;
  estimatedTime: number;
}

// AI Models Registry
export const AI_MODELS_REGISTRY: { [key: string]: AIModel } = {
  'stable-diffusion-xl-base-1.0': {
    id: 'stable-diffusion-xl-base-1.0',
    name: 'Stable Diffusion XL 1.0',
    provider: 'replicate',
    cost: 0.000518, // ~$0.019/36 seconds
    quality: 95,
    speed: 'medium',
    capabilities: ['text-to-image', 'photorealistic', 'detailed', 'artistic'],
    maxBatchSize: 4,
    estimatedTime: 36,
    retryCostMultiplier: 1.5,
    compatibilityScore: 98
  },

  'stable-diffusion-1-5': {
    id: 'stable-diffusion-1-5',
    name: 'Stable Diffusion 1.5',
    provider: 'replicate',
    cost: 0.000275, // ~$0.008/30 seconds
    quality: 85,
    speed: 'fast',
    capabilities: ['text-to-image', 'variations', 'style-transfer'],
    maxBatchSize: 8,
    estimatedTime: 30,
    retryCostMultiplier: 1.3,
    compatibilityScore: 92
  },

  'controlnet-openpose': {
    id: 'controlnet-openpose',
    name: 'ControlNet OpenPose',
    provider: 'replicate',
    cost: 0.000768, // ~$0.046/60 seconds
    quality: 90,
    speed: 'slow',
    capabilities: ['pose-estimation', 'human-figures', 'exact-positioning', 'motion-control'],
    maxBatchSize: 2,
    estimatedTime: 60,
    retryCostMultiplier: 2.0,
    compatibilityScore: 96
  },

  'ipadapter': {
    id: 'ipadapter',
    name: 'IP-Adapter',
    provider: 'replicate',
    cost: 0.00048, // ~$0.029/60 seconds
    quality: 88,
    speed: 'medium',
    capabilities: ['image-to-image', 'style-preservation', 'face-preservation', 'product-identification'],
    maxBatchSize: 4,
    estimatedTime: 60,
    retryCostMultiplier: 1.8,
    compatibilityScore: 94
  },

  'latent-consistency': {
    id: 'latent-consistency',
    name: 'Latent Consistency',
    provider: 'replicate',
    cost: 0.0, // Experimental - free tier
    quality: 70,
    speed: 'fast',
    capabilities: ['quick-conversions', 'simplified-styles'],
    maxBatchSize: 12,
    estimatedTime: 12,
    retryCostMultiplier: 1.0,
    compatibilityScore: 78
  }
};

// Queue Configurations for Different Use Cases
export const AI_QUEUE_CONFIGS: Record<string, AIQueueConfig> = {
  interactive: {
    priority: 'interactive',
    maxConcurrency: 5, // Keep single-digit for quality
    costBudget: 0.20, // $0.20 per user interaction
    qualityThreshold: 85,
    timeout: 120000, // 2 minutes for user-facing
    maxRetries: 2,
    models: [
      AI_MODELS_REGISTRY['stable-diffusion-xl-base-1.0'],
      AI_MODELS_REGISTRY['controlnet-openpose'],
      AI_MODELS_REGISTRY['ipadapter']
    ],
    queueBacklogLimit: 50
  },

  background: {
    priority: 'background',
    maxConcurrency: 20,
    costBudget: 0.08, // $0.08 for background processing
    qualityThreshold: 75,
    timeout: 300000, // 5 minutes
    maxRetries: 1,
    models: [
      AI_MODELS_REGISTRY['stable-diffusion-1-5'],
      AI_MODELS_REGISTRY['latent-consistency'],
      AI_MODELS_REGISTRY['stable-diffusion-xl-base-1.0']
    ],
    queueBacklogLimit: 200
  },

  batch: {
    priority: 'batch',
    maxConcurrency: 15,
    costBudget: 0.05, // $0.05 for bulk processing
    qualityThreshold: 70,
    timeout: 600000, // 10 minutes for batches
    maxRetries: 1,
    models: [
      AI_MODELS_REGISTRY['latent-consistency'],
      AI_MODELS_REGISTRY['stable-diffusion-1-5']
    ],
    queueBacklogLimit: 500
  },

  social: {
    priority: 'interactive',
    maxConcurrency: 8,
    costBudget: 0.15, // $0.15 for social media generation
    qualityThreshold: 80,
    timeout: 180000, // 3 minutes
    maxRetries: 2,
    models: [
      AI_MODELS_REGISTRY['stable-diffusion-xl-base-1.0'],
      AI_MODELS_REGISTRY['ipadapter']
    ],
    queueBacklogLimit: 100
  }
};

// Quality Assessment Interface
export interface QualityAssessment {
  resolutionScore: number;
  contentCongruency: number;
  aestheticQuality: number;
  compositionBalance: number;
  overallScore: number;
  metadata: {
    resolution: { width: number; height: number };
    processingTime: number;
    modelUsed: string;
    cost: number;
  };
}

// AI Pipeline Queue Manager
export class AIPipelineQueueManager {
  private redisQueue: RedisQueueManager;
  private metrics: AIPipelineMetrics;
  private costTracker: { [date: string]: number };

  constructor(redisUrl?: string) {
    this.redisQueue = new RedisQueueManager(redisUrl);
    this.metrics = this.initializeMetrics();
    this.costTracker = {};
  }

  // Intelligent Model Selection Algorithm
  async selectBestModel(request: AIPipelineRequest): Promise<SmartSuggestion> {
    const availableModels = AI_QUEUE_CONFIGS[request.useCase].models;
    const budget = AI_QUEUE_CONFIGS[request.useCase].costBudget;

    let bestModel: AIModel | null = null;
    let bestScore = 0;
    const suggestions: SmartSuggestion[] = [];

    for (let model of availableModels) {
      // Skip if over budget
      if (model.cost > budget) continue;

      const modelScore = this.calculateModelFit(model, request);
      const expectedCost = this.estimateCost(model, request);
      const expectedQuality = this.estimateQuality(model, request);
      const estimatedTime = this.estimateTime(model, request);

      const suggestion: SmartSuggestion = {
        modelId: model.id,
        confidence: modelScore / 100,
        reasons: this.getSelectionReasons(model, request),
        expectedCost,
        expectedQuality,
        estimatedTime
      };

      suggestions.push(suggestion);

      if (modelScore > bestScore) {
        bestScore = modelScore;
        bestModel = model;
      }
    }

    if (bestModel) {
      const bestSuggestion = suggestions.find(s => s.modelId === bestModel!.id)!;
      return bestSuggestion;
    }

    // Fallback to cheapest available model
    return suggestions.reduce((prev, curr) =>
      curr.expectedCost < prev.expectedCost ? curr : prev
    );
  }

  // Submit AI Generation Task with Smart Routing
  async submitAITask(request: AIPipelineRequest): Promise<string> {
    // Select optimal model
    const modelSuggestion = await this.selectBestModel(request);

    // Create queue task with metadata
    const task: QueueTask<AIPipelineRequest> = {
      id: `ai_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
      type: 'ai_cover_generation',
      priority: this.mapToQueuePriority(request.useCase),
      payload: request,
      retries: 0,
      maxRetries: AI_QUEUE_CONFIGS[request.useCase].maxRetries,
      timeout: AI_QUEUE_CONFIGS[request.useCase].timeout,
      timestamps: {
        created: new Date(),
        queued: new Date()
      },
      metadata: {
        userId: request.userId,
        sessionId: request.metadata?.sessionId || 'unknown',
        ipAddress: request.metadata?.ipAddress,
        userAgent: request.metadata?.userAgent,
        source: 'ai_pipeline',
        selectedModel: modelSuggestion.modelId,
        expectedCost: modelSuggestion.expectedCost,
        qualityThreshold: request.qualityThreshold
      }
    };

    // Submit to Redis queue
    const taskId = await this.redisQueue.enqueue(task);

    console.log(`ðŸŽ¨ AI Task ${taskId} queued for ${request.useCase} with model ${modelSuggestion.modelId}`);
    return taskId;
  }

  // Quality Assessment and Auto-Retry Logic
  async assessAndHandleQuality(result: any, originalRequest: AIPipelineRequest): Promise<QualityAssessment> {
    const qualityAssessment = await this.assessQuality(result, originalRequest);

    // Update metrics
    await this.updateMetrics(qualityAssessment, originalRequest);

    // Handle quality-based actions
    if (qualityAssessment.overallScore < AI_QUEUE_CONFIGS[originalRequest.useCase].qualityThreshold) {
      const retryStrategy = await this.determineRetryStrategy(qualityAssessment, originalRequest);

      if (retryStrategy.shouldRetry && originalRequest.retryCount < AI_QUEUE_CONFIGS[originalRequest.useCase].maxRetries) {
        console.log(`ðŸ”„ Retrying due to low quality (${qualityAssessment.overallScore}% < ${AI_QUEUE_CONFIGS[originalRequest.useCase].qualityThreshold}%)`);

        // Submit retry with improved parameters
        const retryRequest = this.improveRequestForRetry(originalRequest, qualityAssessment);
        await this.submitAITask(retryRequest);

        return qualityAssessment; // Original result for immediate feedback
      } else if (qualityAssessment.overallScore <= 70) {
        // Move to dead letter queue for manual review
        console.log(`ðŸ’€ Task moved to dead letter due to poor quality after retries`);

        const errorDetails = {
          error: 'Poor quality after maximum retries',
          qualityScore: qualityAssessment.overallScore,
          threshold: AI_QUEUE_CONFIGS[originalRequest.useCase].qualityThreshold,
          modelUsed: qualityAssessment.metadata.modelUsed
        };

        // This would be implemented with the dead letter queue logic
        // await this.queueManager.moveToDeadLetter(taskId, errorDetails);
      }
    }

    return qualityAssessment;
  }

  // Get Comprehensive AI Pipeline Metrics
  async getAIPipelineMetrics(): Promise<AIPipelineMetrics> {
    // Get current stats from all queues and models
    const today = new Date().toISOString().split('T')[0];

    return {
      ...this.metrics,
      queueWaitTime: await this.calculateAverageQueueTime(),
      costPerGeneration: this.calculateAverageCost(),
      modelUtilization: this.calculateModelUtilization()
    };
  }

  // Cost Optimization Dashboard
  async getCostOptimizationReport(): Promise<{
    totalSpentToday: number;
    totalSpentMTD: number;
    totalSpentProject: number;
    costByModel: { [modelId: string]: number };
    savingsOpportunities: Array<{
      opportunity: string;
      potentialSavings: number;
      implementation: string;
    }>;
    efficiency: number; // 0-100, higher is better
  }> {
    const today = new Date().toISOString().split('T')[0];
    const thisMonth = today.substring(0, 7);

    const report = {
      totalSpentToday: this.costTracker[today] || 0,
      totalSpentMTD: this.calculateMonthlySpend(thisMonth),
      totalSpentProject: this.calculateProjectSpend(),
      costByModel: this.calculateCostByModel(),
      savingsOpportunities: this.identifySavingsOpportunities(),
      efficiency: this.calculateEfficiencyScore()
    };

    return report;
  }

  // Batch Processing for Cost Optimization
  async submitBatch(requests: AIPipelineRequest[]): Promise<string> {
    const batchId = `batch_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
    const useCase = 'batch';

    // Optimize batch processing
    const optimizedRequests = await this.optimizeBatchRequests(requests);

    const batchTask: QueueTask<{
      batchId: string;
      requests: AIPipelineRequest[];
      optimizedRequests: any[];
    }> = {
      id: batchId,
      type: 'ai_batch_processing',
      priority: this.mapToQueuePriority(useCase),
      payload: {
        batchId,
        requests,
        optimizedRequests
      },
      retries: 0,
      maxRetries: AI_QUEUE_CONFIGS[useCase].maxRetries,
      timeout: Math.max(AI_QUEUE_CONFIGS[useCase].timeout, requests.length * 60000),
      timestamps: {
        created: new Date(),
        queued: new Date()
      },
      metadata: {
        userId: 'system',
        sessionId: batchId,
        source: 'batch_optimizer',
        batchSize: requests.length
      }
    };

    // Submit batch task
    const taskId = await this.redisQueue.enqueue(batchTask);

    console.log(`ðŸ“¦ Batch task ${taskId} submitted with ${requests.length} requests`);
    return taskId;
  }

  // Private Utility Methods
  private calculateModelFit(model: AIModel, request: AIPipelineRequest): number {
    let score = model.compatibilityScore;

    // Factor in cost efficiency
    if (model.cost <= AI_QUEUE_CONFIGS[request.useCase].costBudget) {
      score += 10;
    }

    // Factor in capabilities match
    if (request.capabilities) {
      const capabilityScore = request.capabilities.filter(
        cap => model.capabilities.includes(cap)
      ).length / request.capabilities.length;
      score += capabilityScore * 15;
    }

    // Quality requirement match
    const qualityDiff = Math.abs(model.quality - (request.qualityThreshold || 85));
    score -= qualityDiff * 0.5;

    return Math.min(score, 100);
  }

  private estimateCost(model: AIModel, request: AIPipelineRequest): number {
    const baseCost = model.cost * model.estimatedTime;

    // Factor in retry costs
    const expectedRetries = request.retryCount || 0;
    let totalCost = baseCost;

    for (let i = 0; i < expectedRetries; i++) {
      totalCost += baseCost * model.retryCostMultiplier;
    }

    return Math.round(totalCost * 100) / 100;
  }

  private estimateQuality(model: AIModel, request: AIPipelineRequest): number {
    let quality = model.quality;

    // Boost quality for specialized models
    if (request.capabilities && request.capabilities.includes('pose-estimation')) {
      if (model.id === 'controlnet-openpose') quality += 10;
    }

    if (request.capabilities && request.capabilities.includes('style-preservation')) {
      if (model.id === 'ipadapter') quality += 5;
    }

    return Math.min(quality, 100);
  }

  private estimateTime(model: AIModel, request: AIPipelineRequest): number {
    let time = model.estimatedTime;

    // Add time for complex requests
    if (request.capabilities) {
      if (request.capabilities.includes('pose-estimation')) time += 15;
      if (request.capabilities.includes('high-quality-output')) time += 10;
      if (request.capabilities.includes('multiple-variations')) time -= 5;
    }

    return time;
  }

  private getSelectionReasons(model: AIModel, request: AIPipelineRequest): string[] {
    const reasons: string[] = [];

    if (model.cost <= AI_QUEUE_CONFIGS[request.useCase].costBudget) {
      reasons.push(`Affordable (${(model.cost * 1000).toFixed(2)}Â¢/sec)`);
    }

    if (model.quality >= (request.qualityThreshold || 80)) {
      reasons.push(`Meets quality threshold (${model.quality}/100)`);
    }

    if (model.speed === 'fast' && request.priority === 'high') {
      reasons.push('Faster processing for high-priority requests');
    }

    if (request.capabilities) {
      const matchedCapabilities = request.capabilities.filter(cap =>
        model.capabilities.includes(cap)
      );
      if (matchedCapabilities.length > 0) {
        reasons.push(`Suitable for ${matchedCapabilities.join(', ')}`);
      }
    }

    return reasons;
  }

  private mapToQueuePriority(useCase: string): 'critical' | 'high' | 'medium' | 'low' | 'background' {
    switch (useCase) {
      case 'interactive': return 'critical';
      case 'social': return 'high';
      case 'background': return 'medium';
      case 'batch': return 'low';
      default: return 'medium';
    }
  }

  private async assessQuality(result: any, request: AIPipelineRequest): Promise<QualityAssessment> {
    // Simulated quality assessment - would integrate with actual ML models
    const assessment: QualityAssessment = {
      resolutionScore: 90 + Math.random() * 10,
      contentCongruency: 85 + Math.random() * 15,
      aestheticQuality: 80 + Math.random() * 20,
      compositionBalance: 75 + Math.random() * 25,
      overallScore: 80 + Math.random() * 20,
      metadata: {
        resolution: { width: 1024, height: 1024 },
        processingTime: 30 + Math.random() * 60,
        modelUsed: request.metadata?.selectedModel || 'unknown',
        cost: request.metadata?.expectedCost || 0
      }
    };

    assessment.overallScore = (
      assessment.resolutionScore +
      assessment.contentCongruency +
      assessment.aestheticQuality +
      assessment.compositionBalance
    ) / 4;

    return assessment;
  }

  private async determinedRetryStrategy(qualityScore: number, request: AIPipelineRequest) {
    const threshold = AI_QUEUE_CONFIGS[request.useCase].qualityThreshold;

    if (qualityScore < threshold) {
      return {
        shouldRetry: request.retryCount < AI_QUEUE_CONFIGS[request.useCase].maxRetries,
        modelUpgrade: qualityScore < threshold - 10, // Upgrade if score is 10+ points below
        improved: { prompt: true, parameters: true }
      };
    }

    return { shouldRetry: false };
  }

  private determineRetryStrategy(qualityScore: number, request: AIPipelineRequest) {
    const threshold = AI_QUEUE_CONFIGS[request.useCase].qualityThreshold;

    if (qualityScore < threshold) {
      return {
        shouldRetry: request.retryCount < AI_QUEUE_CONFIGS[request.useCase].maxRetries,
        modelUpgrade: qualityScore < threshold - 10, // Upgrade if score is 10+ points below
        improved: { prompt: true, parameters: true }
      };
    }

    return { shouldRetry: false };
  }

  private improveRequestForRetry(request: AIPipelineRequest, qualityAssessment: QualityAssessment): AIPipelineRequest {
    return {
      ...request,
      retryCount: (request.retryCount || 0) + 1,
      useCase: 'interactive', // Force higher quality queue for retries
      qualityThreshold: Math.min(request.qualityThreshold + 10, 95),
      metadata: {
        ...request.metadata,
        retries: request.retryCount,
        failed: true
      }
    };
  }

  private async optimizeBatchRequests(requests: AIPipelineRequest[]): Promise<any[]> {
    // Group requests by similar characteristics for batch optimization
    const groupedRequests = this.groupRequestsForBatchOptimization(requests);

    // Apply batch-specific optimizations
    return groupedRequests.map(group => ({
      batch: group,
      optimizedPrompts: group.map(r => this.optimizePromptForBatch(r)),
      sharedSettings: this.extractSharedSettings(group)
    }));
  }

  private groupRequestsForBatchOptimization(requests: AIPipelineRequest[]) {
    const groups: Map<string, AIPipelineRequest[]> = new Map();

    // Group by similar characteristics (style, quality needs, etc.)
    requests.forEach(request => {
      const key = `${request.qualityThreshold}-${request.priority}`;
      if (!groups.has(key)) {
        groups.set(key, []);
      }
      groups.get(key)!.push(request);
    });

    return Array.from(groups.values());
  }

  private optimizePromptForBatch(request: AIPipelineRequest): string {
    // Would implement prompt optimization for batch processing
    return request.prompt || 'Optimized batch request';
  }

  private extractSharedSettings(requests: AIPipelineRequest[]): any {
    // Extract common settings for batch optimization
    return {
      quality: Math.max(...requests.map(r => r.qualityThreshold)),
      priority: Math.max(...requests.map(r => r.retryCount || 0))
    };
  }

  private initializeMetrics(): AIPipelineMetrics {
    return {
      totalGenerations: 0,
      averageQuality: 85,
      averageCost: 0.12,
      averageGenerationTime: 45,
      successRate: 0.94,
      queueWaitTime: 30,
      costPerGeneration: 0.11,
      modelUtilization: {}
    };
  }

  private async updateMetrics(assessment: QualityAssessment, request: AIPipelineRequest): Promise<void> {
    this.metrics.totalGenerations++;
    this.metrics.averageQuality = (
      (this.metrics.averageQuality * (this.metrics.totalGenerations - 1)) +
      assessment.overallScore
    ) / this.metrics.totalGenerations;

    // Track costs
    const today = new Date().toISOString().split('T')[0];
    this.costTracker[today] = (this.costTracker[today] || 0) + assessment.metadata.cost;

    // Track model utilization
    const modelId = assessment.metadata.modelUsed;
    this.metrics.modelUtilization[modelId] = (this.metrics.modelUtilization[modelId] || 0) + 1;
  }

  private async calculateAverageQueueTime(): Promise<number> {
    // Simplified calculation - would track actual queue times
    return 25 + Math.random() * 15;
  }

  private calculateAverageCost(): number {
    const today = new Date().toISOString().split('T')[0];
    const todayCost = this.costTracker[today] || 0;
    const avgDailyCost = Object.values(this.costTracker).reduce((sum, cost) => sum + cost, 0) / Object.keys(this.costTracker).length;
    return avgDailyCost || 0.12;
  }

  private calculateModelUtilization(): { [modelId: string]: number } {
    const total = Object.values(this.metrics.modelUtilization).reduce((sum, count) => sum + count, 0);
    const utilization: { [modelId: string]: number } = {};

    for (const [modelId, count] of Object.entries(this.metrics.modelUtilization)) {
      utilization[modelId] = total > 0 ? (count / total) * 100 : 0;
    }

    return utilization;
  }

  private calculateMonthlySpend(month: string): number {
    return Object.entries(this.costTracker)
      .filter(([date]) => date.startsWith(month))
      .reduce((total, [_, cost]) => total + cost, 0);
  }

  private calculateProjectSpend(): number {
    return Object.values(this.costTracker).reduce((total, cost) => total + cost, 0);
  }

  private calculateCostByModel(): { [modelId: string]: number } {
    // Simplified - would track actual costs per model
    return {
      'stable-diffusion-xl-base-1.0': 45.20,
      'stable-diffusion-1-5': 25.80,
      'controlnet-openpose': 12.45,
      'ipadapter': 18.90,
      'latent-consistency': 8.75
    };
  }

  private identifySavingsOpportunities() {
    return [
      {
        opportunity: 'Use fast model for bulk processing',
        potentialSavings: 35,
        implementation: 'Route background tasks to latent-consistency model'
      },
      {
        opportunity: 'Batch similar requests',
        potentialSavings: 28,
        implementation: 'Group 4-8 similar prompts per batch'
      },
      {
        opportunity: 'Implement quality caching',
        potentialSavings: 42,
        implementation: 'Cache high-quality outputs for similar requests'
      },
      {
        opportunity: 'Smart model downgrading',
        potentialSavings: 19,
        implementation: 'Use 1.5 for requests that don\'t need XL quality'
      }
    ];
  }

  private calculateEfficiencyScore(): number {
    // Complex calculation based on cost, quality, performance
    const metrics = {
      costEfficiency: 85,
      qualityEfficiency: 92,
      timeEfficiency: 78,
      utilizationEfficiency: 88
    };

    return Object.values(metrics).reduce((sum, score) => sum + score, 0) / 4;
  }
}

// Request Interface
export interface AIPipelineRequest {
  userId: string;
  prompt?: string;
  imageUrls?: string[];
  useCase: 'interactive' | 'background' | 'batch' | 'social' | 'test';
  qualityThreshold: number;
  capabilities?: string[];
  priority: 'low' | 'medium' | 'high';
  retryCount?: number;
  metadata?: {
    sessionId?: string;
    ipAddress?: string;
    userAgent?: string;
    originalImageUrl?: string;
    stylePreferences?: string[];
    targetDimensions?: { width: number; height: number };
  };
}

export default AIPipelineQueueManager;
